{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 5.2 EKF Localization\n\nThe Kalman filter is one of the best studied techniques for filtering and prediction of linear systems. Among its virtues, it provides a way to overcome the occasional un-observability problem of the Least Squares approach. Nevertheless, it makes a strong assumption that the two involved process equations (state transition and observation) are linear. \n\nUnfortunately, you should already know that our system of measurements (i.e. the observation function) and movement (i.e. pose composition) are non-linear.\nTherefore, this notebook focuses from the get-go on the **Extended Kalman Filter**, which is adapted to work with non-linear systems.\n\nThe EKF algorithm consists of 2 phases: **prediction** and **correction**.\n\n$$\n  \\begin{aligned}\n      \\verb!def !& \\verb!ExtendedKalmanFilter!(\\mu_{t-1},\\Sigma_{t-1}, u_t, z_t): \\\\\n      & \\textbf{Prediction.} \\\\\n      & \\bar\\mu_t = g(\\mu_{t-1}, u_t) = \\mu_{t-1} \\oplus u_t &\\text{(1. Pose prediction)}\\\\\n      & \\bar\\Sigma_t = G_t \\Sigma_{t-1} G_t^T + R_t &\\text{(2. Uncertainty of prediction)}\\\\\n      & \\textbf{Correction.} \\\\\n      & K_t = \\bar\\Sigma_t H^T_t (H_t \\bar\\Sigma_t H^T_t + Q_t)^{-1} &\\text{(3. Kalman gain)}\\\\\n      & \\mu_t = \\bar\\mu_t + K_t (z_t - h(\\bar\\mu_t)) &\\text{(4. Pose estimation)}\\\\\n      & \\Sigma_t = (I - K_t H_t) \\bar\\Sigma_t &\\text{(5. Uncertainty of estimation)}\\\\\n      & \\verb!return ! \\mu_t, \\Sigma_t\n  \\end{aligned}\n$$\n\nNotice that $R_t$ is the covariance of the motion $u_t$ in the coordinate system of the predicted pose $(\\bar x_t)$, then (Note: $J_2$ is our popular Jacobian for the motion command, you could also use $J_1$):\n\n$$R_t = J_2 \\Sigma_{u_t} J_2^T \\quad\\text{with}\\quad J_2 = \\frac{\\partial g(\\mu_{t-1}, u_t)}{\\partial u_t}$$\n\nWhere:\n\n  - $(\\mu_t, \\Sigma_t)$ represents our robots pose.\n  - $(u_t, \\Sigma_{u_t})$ is the movement command received, and its respective uncertainty.\n  - $(z_t, Q_t)$ are the observations taken, and their covariance.\n  - $G_t$ and $H_t$ are the Jacobians of the motion model and the observation model respectively:\n\n$$G_t = \\frac{\\partial g(\\mu_{t-1}, u_t)}{\\partial x_{t-1}}, \\qquad H_t = \\frac{\\partial h(\\bar\\mu_t)}{\\partial x_t}$$"},{"metadata":{},"cell_type":"markdown","source":"In this notebook we are going to play with the EKF localization algorithm using a map of landmarks and a sensor providing range and bearing measurements from the robot pose to such landmarks. Concretely, **we are going to**:\n1. Implement a class modeling a **range and bearing sensor** able to take measurements to landmarks.\n2. Complete a class that implements the robot behavior after completing **movement commands**.\n3. Implement the **Jacobian of the observation model**.\n4. With the previous building blocks, implement our own **EKF filter** and see it in action.\n5. Finally, we are going to consider a more **realistic sensor** with a given Field of View and a maximum operational range. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# IMPORTS\nimport numpy as np\nfrom numpy import random\nfrom numpy import linalg\nimport matplotlib\nmatplotlib.use('TkAgg')\nfrom matplotlib import pyplot as plt\n\nimport sys\nsys.path.append(\"..\")\nfrom utils.AngleWrap import AngleWrapList\nfrom utils.PlotEllipse import PlotEllipse\nfrom utils.Drawings import DrawRobot, drawFOV, drawObservations\nfrom utils.Jacobians import J1, J2\nfrom utils.tcomp import tcomp","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **<span style=\"color:green\"><b><i>ASSIGNMENT 1: Getting an observation to a random landmark</i></b></span>** \n\n\nWe are going to implement the `Sensor()` class modelling a range and bearing sensor. Recall that the observation model of this type of sensos is:\n\n$$\nz_i= \\begin{bmatrix} d_i \\\\ \\theta_i \\end{bmatrix}=h(m_i,x)=\n\\begin{bmatrix} \\sqrt{(x_i-x)^2+(y_i-y)^2} \\\\ atan\\left(\\frac{y_i-y}{x_i-x}\\right) - \\theta \\end{bmatrix}+w_i\n$$\n\nwhere $m_i=[x_i,y_i]$ are the landmark coordinates in the world frame, $x=[x,y,\\theta]$ is the robot pose, and the noise $w_i$ follows a Gaussian distribution with zero mean and covariance matrix:\n\n$$\\Sigma_S = \n    \\begin{bmatrix}\n        \\sigma^2_r & 0 \\\\\n        0 & \\sigma^2_{\\theta}\n    \\end{bmatrix}\n$$\n\nFor that, complete the following methods:\n- `observe()`: which, given a real robot pose (`from_pose`), returns the measurments to the landmarks in the map (`world`). If `noisy=true`, then a random gaussian noise with zero mean and covariance $\\Sigma_S$ (`cov`) is added to each measurement. *Hint you can use [`random.randn()`](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.randn.html) for that.*\n\n- `random_observation()`: that, given again the robot pose (`from_pose`), randomly selects a landmark from the map (`world`) and returns an observation from the range-bearing sensor using the `observe()` method previously implemented. The `noisy` argument is just passed to `observe()`.  *Hint: to randomly select a landmark, use [`randint()`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randint.html).*"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Sensor():\n    def __init__(self, cov):\n        \"\"\"\n        Args:\n            cov: covariance of the sensor.\n        \"\"\"\n        self.cov = cov\n    \n    def observe(self, from_pose, world, noisy=True, flatten=True):\n        \"\"\"Calculate observation relative to from_pose\n\n        Args:\n            from_pose: Position(real) of the robot which takes the observation\n            world: List of world coordinates of some landmarks\n            noisy: Flag, if true then add noise (Exercise 2)\n\n        Returns:\n                Numpy array of polar coordinates of landmarks from the perspective of our robot\n                They are organised in a vertical vector ls = [d_0 , a_0, d_1, ..., a_n]'\n                Dims (2*n_landmarks, 1) \n        \"\"\"\n        delta = world - from_pose[0:2]\n\n        z = np.empty_like(delta)\n        z[0, :] = np.sqrt(np.power(delta[0,:],2)+np.power(delta[1,:],2))\n        z[1, :] = np.arctan2(delta[1,:],delta[0,:]) - from_pose[2]\n        z[1, :] = AngleWrapList(z[1, :])\n\n        if noisy: \n            z += np.sqrt(self.cov)@ np.random.randn(z.shape[0], z.shape[1])\n\n        if flatten:\n            return np.vstack(z.flatten('F'))\n        else:\n            return z\n        \n    def random_observation(self, from_pose, world, noisy=True):\n        \"\"\" Get an observation from a random landmark \n            \n            Args: Same as observe().\n                \n            Returns:\n                z: Numpy array of obs. in polar coordinates\n                landmark: Index of the randomly selected landmark in the world map\n                    Although it is only one index, you should return it as\n                    a numpy array.\n        \"\"\"\n        n_landmarks = world.shape[1]\n        rand_idx = np.random.randint(0,n_landmarks)\n        world = world[:, [rand_idx]]\n        \n        z = self.observe(from_pose, world, noisy)\n\n        return z, np.array([rand_idx])","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can use the code cell below **to test your implementation**."},{"metadata":{"trusted":true},"cell_type":"code","source":"# TRY IT!\nseed = 0\nnp.random.seed(seed)\n\n# Sensor characterization\nSigmaR = 1 # Standard deviation of the range\nSigmaB = 0.7 # Standard deviation of the bearing\nQ = np.diag([SigmaR**2, SigmaB**2]) # Cov matrix\n\nsensor = Sensor(Q)\n\n# Map\nSize = 50.0\nNumLandmarks = 3\nMap = Size*2*random.rand(2,NumLandmarks)-Size\n\n# Robot true pose\ntrue_pose = np.vstack([-Size+Size/3, -Size+Size/3, np.pi/2])\n\n# Take a random measurement\nnoisy = False\nz = sensor.random_observation(true_pose, Map, noisy)\n\nnoisy = True\nnoisy_z = sensor.random_observation(true_pose, Map, noisy)\n\n# Take observations to every landmark in the map\nzs = sensor.observe(true_pose, Map, noisy)\n\nprint('Measurement:\\n' + str(z))\nprint('Noisy measurement:\\n' + str(noisy_z))\nprint('Measurements to every landmark in the map:\\n' + str(zs))","execution_count":11,"outputs":[{"output_type":"stream","text":"Measurement:\n(array([[53.76652662],\n       [-0.79056712]]), array([0]))\nNoisy measurement:\n(array([[64.73997127],\n       [-0.81342958]]), array([2]))\nMeasurements to every landmark in the map:\n[[ 5.51319938e+01]\n [-1.10770618e+00]\n [ 6.04762304e+01]\n [-1.46219661e+00]\n [ 6.23690518e+01]\n [-5.72010701e-02]]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:blue\">Expected output</span>\n\n```\nMeasurement:\n(array([[53.76652662],\n       [-0.79056712]]), array([0]))\nNoisy measurement:\n(array([[64.73997127],\n       [-0.81342958]]), array([2]))\nMeasurements to every landmark in the map:\n[[ 5.51319938e+01]\n [-1.10770618e+00]\n [ 6.04762304e+01]\n [-1.46219661e+00]\n [ 6.23690518e+01]\n [-5.72010701e-02]]\n ```"},{"metadata":{},"cell_type":"markdown","source":"### **<span style=\"color:green\"><b><i>ASSIGNMENT 2: Simulating the robot motion</i></b></span>** \nIn the robot motion chapter we commanded a mobile robot to follow a squared trajectory. We provide here the `Robot` class that implements:\n- how the robot pose evolves after executing a motion command (`step()` method), and\n- the functionality needed to graphically show its ideal pose (`pose`), true pose (`true_pose`) and estimated pose (`xEst`) in the `draw()` function.\n\nYour mission is to complete the `step()`method by adding random noise to each motion command (`noisy_u`) based on the following covariance matrix, and update the true robot pose (`true_pose`):\n\n$$\\Sigma_{u_t} = \n    \\begin{bmatrix}\n        \\sigma^2_{\\Delta x} & 0 & 0\\\\\n        0 & \\sigma^2_{\\Delta y} & 0\\\\\n        0 & 0 & \\sigma^2_{\\Delta \\theta}\n    \\end{bmatrix}\n$$ \n\n*Hint: Recall again the [`random.randn()`](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.randn.html) function.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Robot():\n    def __init__(self, true_pose, cov_move):\n        # Robot description (Starts as perfectly known)\n        self.pose = true_pose\n        self.true_pose = true_pose\n        self.cov_move = cov_move\n\n        # Estimated pose and covariance\n        self.xEst = true_pose\n        self.PEst = np.zeros((3, 3))\n        \n    def step(self, u):\n        self.pose = tcomp(self.pose,u) # New pose without noise\n        noise = np.sqrt(self.cov_move)@random.randn(3,1) # Generate noise\n        noisy_u = u+noise #  Apply noise to the control action\n        self.true_pose = tcomp(self.true_pose,noisy_u) #  New noisy pose (real robot pose)\n        \n    def draw(self, fig, ax):\n        DrawRobot(fig, ax, self.pose, color='r')\n        DrawRobot(fig, ax, self.true_pose, color='b')\n        DrawRobot(fig, ax, self.xEst, color='g')\n        PlotEllipse(fig, ax, self.xEst, self.PEst, 4, color='g')","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is time **to test** your `step()` function!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Robot base characterization\nSigmaX = 0.8 # Standard deviation in the x axis\nSigmaY = 0.8 # Standard deviation in the y axis\nSigmaTheta = 0.1 # Bearing standar deviation\nR = np.diag([SigmaX**2, SigmaY**2, SigmaTheta**2]) # Cov matrix\n\n# Create the Robot object\ntrue_pose = np.vstack([2,3,np.pi/2])\nrobot = Robot(true_pose, R)\n\n# Perform a motion command\nu = np.vstack([1,2,0])\nnp.random.seed(0)\nrobot.step(u)\n\nprint('robot.true_pose.T:' + str(robot.true_pose.T) + '\\'')","execution_count":13,"outputs":[{"output_type":"stream","text":"robot.true_pose.T:[[-0.32012577  5.41124188  1.66867013]]'\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:blue\">Expected output</span>\n\n```\nrobot.true_pose.T:[[-0.32012577  5.41124188  1.66867013]]'\n```"},{"metadata":{},"cell_type":"markdown","source":"### **<span style=\"color:green\"><b><i>ASSIGNMENT 3:  Jacobians of the observation model</i></b></span>** \n\nGiven that the position of the landmarks in the map is known, we can use this information in a Kalman filter, in our case an EKF. For that we need to implement the **Jacobians of the observation model**, as required by the correction step of the filter. \n\nImplement the function `getObsJac()`that given:\n- the predicted pose in the first step of the Kalman filter, \n- a number of observed landmarks, and \n- the map, \n\nreturns such Jacobian. Recall that, for each observation to a landmark:\n\n$$\n\\nabla H = \\frac{\\partial h}{\\partial \\{x,y,\\theta \\}} =\n\\begin{bmatrix}\n    -\\frac{x_i - x}{d} & - \\frac{y_i - y}{d} & 0 \\\\\n    \\frac{y_i - y}{d^2} & -\\frac{x_i - x}{d^2} & -1\n\\end{bmatrix}_{2 \\times 3}\n$$\n\nRecall that $[x_i,y_i]$ is the position of the $i^{th}$ landmark in the map, $[x,y]$ is the robot predicted pose, and $d$ the distance such to the landmark. This way, the resultant Jacobian dimensions are $(\\#observed\\_landmarks \\times 2, 3)$, that is, the Jacobians are stacked vertically to form the matrix $H$."},{"metadata":{"trusted":true},"cell_type":"code","source":"def getObsJac(xPred, lm, Map): \n    \"\"\" Obtain the Jacobian for all observations.\n\n        Args:\n            xPred: Position of our robot at which Jac is evaluated.\n            lm: Numpy array of observations to a number of landmarks (indexes in map)\n            Map: Map containing the actual positions of the observations.\n\n        Returns:\n            jH: Jacobian matrix (2*n_landmaks, 3) \n    \"\"\"\n    n_land = len(lm)\n    jH = np.empty((2*n_land,3))\n    \n    for i in range(n_land):\n        # Auxiliary variables\n        dx = Map[0,lm[i]] -xPred[0]\n        dy = Map[1,lm[i]] -xPred[1]\n        d = np.sqrt(np.power(dx,2)+np.power(dy,2))\n        d2 = np.power(d,2)\n        \n        ii = 2*i\n\n        # Build the Jacobian\n        jH[ii:ii+2,:] = [\n            [-dx/d, -dy/d, 0],\n            [dy/d2, -dx/d2, -1]\n        ]\n\n    return jH","execution_count":18,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Time **to check** your function!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# TRY IT!\n\nobserved_landmarks = np.array([0,2])\nxPred = np.vstack([-Size+Size/3, -Size+Size/3, np.pi/2]) # Robot predicted pose\njH = getObsJac(xPred, observed_landmarks, Map) # Retrieve the evaluated observation jacobian\n\nprint ('Jacobian dimensions: ' + str(jH.shape) )\nprint ('jH:' + str(jH))","execution_count":19,"outputs":[{"output_type":"stream","text":"Jacobian dimensions: (4, 3)\njH:[[-0.71075232 -0.70344235  0.        ]\n [ 0.01308328 -0.01321923 -1.        ]\n [-0.67304061 -0.73960552  0.        ]\n [ 0.01141455 -0.01038723 -1.        ]]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:blue\">Expected output:</span>\n\n```\nJacobian dimensions: (4, 3)\njH:[[-0.71075232 -0.70344235  0.        ]\n [ 0.01308328 -0.01321923 -1.        ]\n [-0.67304061 -0.73960552  0.        ]\n [ 0.01141455 -0.01038723 -1.        ]]\n```"},{"metadata":{},"cell_type":"markdown","source":"### **<span style=\"color:green\"><b><i>ASSIGNMENT 4: Completing the EKF</i></b></span>** \n\nCongratulations! You now have all the building blocks needed to implement an EKF filter (both prediction and correction steps) for localizating the robot and show the estimated pose and its uncertainty. \n\nFor doing that, complete the `EKFLocalization()` function below, which returns:\n- the estimated pose (`xEst`), and \n- its associated uncertainty (`PEst`), \n\ngiven:\n- the previous estimations (`self.xEst` and `self.PEst` stored in `robot`), \n- the features of the sensor (`sensor`), \n- the movement command provided to the robot (`u`), \n- the observations done (`z`), \n- the indices of the observed landmarks (`landmark`), and \n- the map of the environment (`Map`)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def EKFLocalization(robot, sensor, u, z, landmark, Map):\n    \"\"\" Implement the EKF algorithm for localization\n        \n        Args:\n            robot: Robot base (contains the state: xEst and PEst)\n            sensor: Sensor of our robot.\n            u: Movement command\n            z: Observations received\n            landmark: Indices of landmarks observed in z\n            Map: Array with landmark coordinates in the map\n            \n        Returns:\n            xEst: New estimated pose\n            PEst: Covariance of the estimated pose\n    \"\"\"\n    \n    # Prediction \n    xPred = tcomp(robot.xEst, u)\n    G = J1(robot.xEst, u)\n    j2 = J2(robot.xEst, _)\n    PPred = G@robot.PEst@G.T + j2@robot.cov_move@j2.T \n    \n    # Correction (You need to compute the gain k and the innovation z-z_p) \n    if landmark.shape[0] > 0: \n        H = getObsJac(xPred, landmark, Map)# Observation Jacobian\n        K = PPred @H.T @ np.linalg.inv(H@PPred@H.T+ np.diag(np.tile(np.diag(sensor.cov),len(landmark))) )\n        xEst = xPred + K@(z-sensor.observe(xPred,Map[:,landmark]))  # New estimated pose\n        PEst = (np.eye(3,3)-K@H)@PPred # New estimated Jacobian\n    else:\n        xEst = xPred\n        PEst = PPred\n    \n    return xEst, PEst","execution_count":36,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can **validate your code** with the code cell below."},{"metadata":{"trusted":true},"cell_type":"code","source":"# TRY IT!\n\nnp.random.seed(2)\n\n# Create the map\nMap=Size*2*random.rand(2,20)-Size\n\n# Create the Robot object\ntrue_pose = np.vstack([2,3,0])\nR = np.diag([0.1**2, 0.1**2, 0.01**2]) # Cov matrix\nrobot = Robot(true_pose, R)\n\n# Perform a motion command\nu = np.vstack([10,0,0])\nrobot.step(u)\n\n# Get an observation to a landmark\nnoisy = True\nnoisy_z, landmark_index = sensor.random_observation(true_pose, Map, noisy)\n\n# Estimate the new robot pose using EKF!\nrobot.xEst, robot.PEst = EKFLocalization(robot, sensor, u, noisy_z, landmark_index, Map)\n\n# Show resutls!\nprint('robot.pose.T:' + str(robot.pose.T) + '\\'')\nprint('robot.true_pose.T:' + str(robot.true_pose.T) + '\\'')\nprint('robot.xEst.T:' + str(robot.xEst.T) + '\\'')\nprint('robot.PEst:' + str(robot.PEst.T))","execution_count":37,"outputs":[{"output_type":"stream","text":"robot.pose.T:[[12.  3.  0.]]'\nrobot.true_pose.T:[[ 1.20000010e+01  3.05423526e+00 -3.13508197e-03]]'\nrobot.xEst.T:[[ 1.19628524e+01  2.96476130e+00 -2.15825855e-04]]'\nrobot.PEst:[[ 9.94877200e-03 -4.94253023e-05 -3.18283546e-08]\n [-4.94253023e-05  9.95211532e-03  3.29230513e-08]\n [-3.18283546e-08  3.29230513e-08  9.99795962e-05]]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:blue\">Expected output:</span>\n```\nrobot.pose.T:[[12.  3.  0.]]'\nrobot.true_pose.T:[[ 1.20000010e+01  3.05423526e+00 -3.13508197e-03]]'\nrobot.xEst.T:[[ 1.19586407e+01  2.96047951e+00 -1.48514185e-04]]'\nrobot.PEst:[[ 9.94877200e-03 -4.94253023e-05 -3.18283546e-08]\n [-4.94253023e-05  9.95211532e-03  3.29230513e-08]\n [-3.18283546e-08  3.29230513e-08  9.99795962e-05]]\n```"},{"metadata":{},"cell_type":"markdown","source":"## Playing with EKF\n\nThe following code helps you to see the EKF filter in action!. Press any key on the emerging window to send a motion command to the robot and check how the landmark it observes changes, as well as its ideal, true and estimated poses. \n\n**Notice that** you can change the value of `seed` within the `main()`function to try different executions.\n\n**Example**\n\nThe figure below shown an example of the execution of the EKF localization algorithm with the code implemented until this point.\n\n<figure style=\"text-align:center\">\n  <img src=\"images/fig5-2-1.png\" width=\"500\" alt=\"\">\n  <figcaption>\n      Fig. 1: Execution of the EKF algorithmn for localization, <br/>\n      it shows the true (in blue) and expected (in red) poses, <br/>\n      along the results from localization: pose and ellipse (in green), <br/>\n      the existing landmarks (in cyan), <br/>\n      and each observation made (dotted lines).\n  </figcaption>\n</figure>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def main(robot,\n         sensor,\n         mode='one_landmark',\n         nSteps=20, # Number of motions\n         turning=5, # Number of motions before turning (square path)\n         Size=50.0,\n         NumLandmarks=10):\n    seed = 1\n    np.random.seed(seed)\n    \n    #Create map\n    Map=Size*2*random.rand(2,NumLandmarks)-Size\n    \n    # MATPLOTLIB\n    plt.ion()\n    fig, ax = plt.subplots()\n    plt.plot(Map[0,:],Map[1,:],'sc')\n    plt.axis([-Size-15, Size+15, -Size-15, Size+15])\n    \n    robot.draw(fig, ax)\n    fig.canvas.draw()\n    \n    # MAIN LOOP\n    \n    u = np.vstack([(2*Size-2*Size/3)/turning,0,0]) # control action\n    plt.waitforbuttonpress(-1)    \n    \n    \n    for k in range(0, nSteps-3): # Main loop\n        u[2] = 0\n        if k % turning == turning-1: # Turn?\n            u[2] = -np.pi/2\n\n        robot.step(u)\n        \n        # Get sensor observation/s\n        if mode == 'one_landmark':\n            # DONE (Exercise 4)\n            z, landmark = sensor.random_observation(robot.true_pose, Map)\n            ax.plot(\n                [robot.true_pose[0,0], Map[0,landmark]],\n                [robot.true_pose[1,0], Map[1,landmark]],\n                color='m', linestyle=\"--\", linewidth=.5)\n        elif mode == 'landmarks_in_fov':\n            # DONE (Exercise 5)\n            z, landmark = sensor.observe_in_fov(robot.true_pose, Map)\n            drawObservations(fig, ax, robot.true_pose, Map[:, landmark])\n        \n        robot.xEst, robot.PEst = EKFLocalization(robot, sensor, u, z, landmark, Map)\n\n        # Drawings\n        # Plot the FOV of the robot\n        if mode == 'landmarks_in_fov':\n            h = sensor.draw(fig, ax, robot.true_pose)\n        #end\n\n        robot.draw(fig, ax)\n\n        fig.canvas.draw()\n        plt.waitforbuttonpress(-1)    \n\n        if mode == 'landmarks_in_fov':\n            h.pop(0).remove()\n        fig.canvas.draw()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RUN\nmode = 'one_landmark'\n#mode = 'landmarks_in_fov'\n\nSize=50.0\n\n# Robot base characterization\nSigmaX = 0.8 # Standard deviation in the x axis\nSigmaY = 0.8 # Standard deviation in the y axis\nSigmaTheta = 0.1 # Bearing standar deviation\nR = np.diag([SigmaX**2, SigmaY**2, SigmaTheta**2]) # Cov matrix\n\ntrue_pose = np.vstack([-Size+Size/3, -Size+Size/3, np.pi/2])\nrobot = Robot(true_pose, R)\n\n# Sensor characterization\nSigmaR = 1 # Standard deviation of the range\nSigmaB = 0.7 # Standard deviation of the bearing\nQ = np.diag([SigmaR**2, SigmaB**2]) # Cov matrix\n\nsensor = Sensor(Q)\n\nmain(robot, sensor, mode=mode, Size=Size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **<span style=\"color:green\"><b><i>ASSIGNMENT 5:  Implementing the FoV of a sensor.</i></b></span>** \n\nSensors exhibit certain physical limitations regarding their field of view (FoV) and maximum operating distance (max. Range). Besides, these devices often do not deliver measurmenets from just one landmark each time, but from all those landmarks in the FoV. \n\nThe `FOVSensor()` class below extends the `Sensor()` one to implement this behaviour. Complete the `observe_in_fov()` method to consider that the sensor can only provide information from the landmkars in a limited range $r_l$ and a limited orientation $\\pm \\alpha$ with respect to the robot pose. For that:\n\n1. Get the observations to every landmark in the map. Use the `observe()` function previously implemented for that, but with the argument `flatten=False`. With that option the function returns the measurements as:\n$$\nz = \\begin{bmatrix} d_1 &  \\cdots &  d_m \\\\ \\theta_1 & \\cdots & \\theta_m \\end{bmatrix}\n$$\n2. Check which observations lay in the sensor FoV and maximum operating distance. *Hint: for that, you can use the [`np.asarray()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.asarray.html) function with the conditions to be fulfilled by the valid measurements inside, and then filter the results with [`np.nonzero()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html)*. \n3. Flatten the resultant matrix `z` to be again a vector, so it has the shape $(2 \\times \\#Observed\\_landmarks,1)$. *Hint: take a look at [`np.ndarray.flatten()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.flatten.html) and choose the proper argument.*\n\nNotice that it could happen that any landmark exists in the field of view of the sensor, so the robot couldn’t gather sensory information in that iteration. This, which is a problem using Least Squares Positioning, is not an issue with EKF. ***Hint: you can change the value of `seed` within the `main()`function to try different executions.***"},{"metadata":{"trusted":false},"cell_type":"code","source":"class FOVSensor(Sensor):\n    def __init__(self, cov, fov, max_range):\n        super().__init__(cov)\n        self.fov = fov\n        self.max_range = max_range\n    \n    def observe_in_fov(self, from_pose, world, noisy=True):\n        \"\"\" Get all observations in the fov\n\n        Args:\n            from_pose: Position(real) of the robot which takes the observation\n            world: List of world coordinates of some landmarks\n            noisy: Flag, if true then add noise (Exercise 2)\n\n        Returns:\n            Numpy array of polar coordinates of landmarks from the perspective of our robot\n            They are organised in a vertical vector ls = [d_0 , a_0, d_1, ..., a_n]'\n            Dims (2*n_landmarks, 1) \n        \"\"\"                \n        # 1. Get observations to every landmark in the map WITHOUT NOISE\n        z = self.observe(None, None, None, None)\n        \n        # 2. Check which ones lay on the sensor FOV\n        angle_limit = self.fov/2 # auxiliar variable\n        feats_idx = None # indices of the valid observations    \n                \n        if noisy:\n            # 1. Get observations to every landmark in the map WITH NOISE\n            z = self.observe(None, None, None, None)\n            \n        z = z[:, feats_idx] # extracts the valid observations from z\n               \n        # 3. Flatten the  resultant vector of measurements so z=[d_1,theta_1,d_2,theta_2,...,d_n,theta_n]   \n        if z.size>0:\n            z = None\n            \n        return z, feats_idx\n    \n    def draw(self, fig, ax, from_pose):\n        \"\"\" Draws the Field of View of the sensor from the robot pose \"\"\"\n        return drawFOV(fig, ax, from_pose, self.fov, self.max_range)   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can now **try** your new and more realistic sensor."},{"metadata":{"trusted":false},"cell_type":"code","source":"# TRY IT!\nnp.random.seed(0)\n\n# Create the sensor object\ncov = np.diag([0.1**2, 0.1**2]) # Cov matrix\nfov = np.pi/2\nmax_range = 2\nsensor = FOVSensor(cov, fov, max_range)\n\n# Create a map with three landmarks\nMap = np.array([[2., 2.5, 3.5, 0.5],[2., 3., 1.5, 3.5]])\n\n# Take an observation of landmarks in FoV\nrobot_pose = np.vstack([1.,2.,0.])\nz, feats_idx = sensor.observe_in_fov(robot_pose, Map)\nprint('z:' +str(z))\n\n# Plot results\nfig, ax = plt.subplots()\nplt.axis([0, 5, 0, 5])\nplt.title('Measuremets to landmarks in sensor FOV')\nplt.plot(Map[0,:],Map[1,:],'sc')\nsensor.draw(fig, ax, robot_pose)\ndrawObservations(fig, ax, robot_pose, Map[:, feats_idx])\nDrawRobot(fig,ax,robot_pose)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:blue\">Expected output:</span>\n    \n```\nz:[[1.17640523]\n [0.1867558 ]\n [1.84279136]\n [0.49027482]]\n```    "},{"metadata":{},"cell_type":"markdown","source":"## Playing with EKF and the new sensor\n\nAnd finally, play with your own FULL implementation of the EKF filter with a more realistic sensor :)\n\n**Example**\n\nThe figure below shows an example of the execution of EKF using information from all the landmarks within the FOV:\n\n<figure style=\"text-align:center\">\n  <img src=\"images/fig5-2-2.png\" width=\"500\" alt=\"\">\n  <figcaption>\n      Fig. 2: Execution of the EKF algorithmn for localization. <br/>\n      Same as in Fig. 1, except now our robot can observe every <br/> \n      lanmark in its f.o.v. \n  </figcaption>\n</figure>"},{"metadata":{"trusted":false},"cell_type":"code","source":"# RUN\n#mode = 'one_landmark'\nmode = 'landmarks_in_fov'\nSize=50.0\n\n# Robot base characterization\nSigmaX = 0.8 # Standard deviation in the x axis\nSigmaY = 0.8 # Standard deviation in the y axis\nSigmaTheta = 0.1 # Bearing standar deviation\nR = np.diag([SigmaX**2, SigmaY**2, SigmaTheta**2]) # Cov matrix\n\ntrue_pose = np.vstack([-Size+Size/3, -Size+Size/3, np.pi/2])\nrobot = Robot(true_pose, R)\n\n# Sensor characterization\nSigmaR = 1 # Standard deviation of the range\nSigmaB = 0.7 # Standard deviation of the bearing\nQ = np.diag([SigmaR**2, SigmaB**2]) # Cov matrix\nfov = np.pi/2 # field of view = 2*alpha\nmax_range = Size # maximum sensor measurement range\n\nsensor = FOVSensor(Q, fov, max_range)\n\nmain(robot, sensor, mode=mode, Size=Size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font color=\"blue\"><b><i>Thinking about it (1)</i></b></font>\n\nHaving completed the EKF implementation, you are ready to **answer the following questions**:\n\n- What are the dimensions of the Jacobians of the observation model (matrix H)?  Why?\n\n    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>En este caso sería de dimensiones 2x3, tiene 2 filas por cada landmark y 3 columnas para poder multiplicar la pose del robot.</i></p>\n\n- Discuss the evolution of the ideal, true and estimated poses when executing the EKF filter (with the initial sensor).\n\n    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Tenemos la pose roja que sería la ideal, la verde es la predicción , que se acerca a la pose azul real del robot, gracais a la corrección del EKF.</i></p>\n\n- Discuss the evolution of the ideal, true and estimated poses when executing the EKF filter (with the sensor implementing a FOV). Pay special attention to their associated uncertainties. \n\n    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n\n- What happens in the EKF filter when the robot performs a motion command, but it is unable to measure distances to any landmark, i.e. they are out of the sensor FOV?\n\n    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}